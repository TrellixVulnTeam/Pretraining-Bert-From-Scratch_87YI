{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import read_squad_examples\n",
    "train_examples = read_squad_examples(\n",
    "            input_file='squad\\\\v1.1\\\\train-v1.1.json', is_training=True, version_2_with_negative=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "qas_id: 5733be284776f4190066117f, question_text: What is in front of the Notre Dame Main Building?, doc_tokens: [Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.], start_position: 32, end_position: 36"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_examples[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test the dataset from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tokenization import (BasicTokenizer, BertTokenizer, whitespace_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('squad\\\\v1.1\\\\train-v1.1.json','r') as f:\n",
    "    data =json.load(f)['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_whitespace(c):\n",
    "    if c == \" \" or c == \"\\t\" or c == \"\\r\" or c == \"\\n\" or ord(c) == 0x202F:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = []\n",
    "for entry in data:\n",
    "    for paragraph in entry[\"paragraphs\"]:\n",
    "        paragraph_text = paragraph[\"context\"]\n",
    "        doc_tokens = []\n",
    "        char_to_word_offset = []\n",
    "        prev_is_whitespace = True\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in paragraph_text:\n",
    "    if is_whitespace(c):\n",
    "        prev_is_whitespace = True\n",
    "    else:\n",
    "        if prev_is_whitespace:\n",
    "            doc_tokens.append(c)\n",
    "        else:\n",
    "            doc_tokens[-1]+=c\n",
    "        prev_is_whitespace = False\n",
    "    char_to_word_offset.append(len(doc_tokens)-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for qa in paragraph[\"qas\"]:\n",
    "    qas_id = qa[\"id\"]\n",
    "    question_text = qa[\"question\"]\n",
    "    start_position = None\n",
    "    end_position = None\n",
    "    orig_answer_text = None\n",
    "    is_impossible = False\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Which Secretary of State attended Notre Dame?'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'answer_start': 185, 'text': 'Condoleezza Rice'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa[\"answers\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    if False:\n",
    "        is_impossible = qa[\"is_impossible\"]\n",
    "    if (len(qa[\"answers\"]) != 1) and (not is_impossible):\n",
    "        raise ValueError(\n",
    "            \"For training, each question should have exactly 1 answer.\")\n",
    "    if not is_impossible:\n",
    "        answer = qa[\"answers\"][0]\n",
    "        orig_answer_text = answer[\"text\"]\n",
    "        answer_offset = answer[\"answer_start\"]\n",
    "        answer_length = len(orig_answer_text)\n",
    "        start_position = char_to_word_offset[answer_offset]\n",
    "        end_position = char_to_word_offset[answer_offset + answer_length - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_text = \" \".join(doc_tokens[start_position:(end_position + 1)])\n",
    "cleaned_answer_text = \" \".join(\n",
    "    whitespace_tokenize(orig_answer_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Condoleezza Rice.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Condoleezza Rice'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_answer_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer('.\\\\vocab\\\\vocab', do_lower_case=True, max_len=512) # for bert large\n",
    "for (example_index, example) in enumerate(train_examples):\n",
    "        query_tokens = tokenizer.tokenize(example.question_text)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(query_tokens) > 60:\n",
    "    query_tokens = query_tokens[0:60]\n",
    "## the original index before tokenizer\n",
    "tok_to_orig_index = []\n",
    "## the index after tokenizer\n",
    "orig_to_tok_index = []\n",
    "all_doc_tokens = []\n",
    "for (i, token) in enumerate(example.doc_tokens):\n",
    "    orig_to_tok_index.append(len(all_doc_tokens))\n",
    "    sub_tokens = tokenizer.tokenize(token)\n",
    "    for sub_token in sub_tokens:\n",
    "        tok_to_orig_index.append(i)\n",
    "        all_doc_tokens.append(sub_token)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import _improve_answer_span\n",
    "is_training = True\n",
    "tok_start_position = None\n",
    "tok_end_position = None\n",
    "if is_training and example.is_impossible:\n",
    "    tok_start_position = -1\n",
    "    tok_end_position = -1\n",
    "if is_training and not example.is_impossible:\n",
    "    tok_start_position = orig_to_tok_index[example.start_position]\n",
    "    if example.end_position < len(example.doc_tokens) - 1:  ## \n",
    "        tok_end_position = orig_to_tok_index[example.end_position + 1] - 1\n",
    "    else:\n",
    "        tok_end_position = len(all_doc_tokens) - 1\n",
    "    (tok_start_position, tok_end_position) = _improve_answer_span(\n",
    "        all_doc_tokens, tok_start_position, tok_end_position, tokenizer,\n",
    "        example.orig_answer_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tokens_for_doc = 512 - len(query_tokens) - 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "158"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_doc_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_doc_tokens = all_doc_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "# We can have documents that are longer than the maximum sequence length.\n",
    "# To deal with this we do a sliding window approach, where we take chunks\n",
    "# of the up to our max length with a stride of `doc_stride`.\n",
    "_DocSpan = collections.namedtuple(  # pylint: disable=invalid-name\n",
    "    \"DocSpan\", [\"start\", \"length\"])\n",
    "doc_spans = []\n",
    "start_offset = 0\n",
    "while start_offset < len(all_doc_tokens):\n",
    "    length = len(all_doc_tokens) - start_offset\n",
    "    if length > max_tokens_for_doc:\n",
    "        length = max_tokens_for_doc\n",
    "    doc_spans.append(_DocSpan(start=start_offset, length=length))\n",
    "    if start_offset + length == len(all_doc_tokens):\n",
    "        break\n",
    "    start_offset += min(length, 126)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (doc_span_index, doc_span) in enumerate(doc_spans):\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add cls and sep to query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokens = []\n",
    "token_to_orig_map = {}\n",
    "token_is_max_context = {}\n",
    "segment_ids = []\n",
    "tokens.append(\"[CLS]\")\n",
    "segment_ids.append(0)\n",
    "for token in query_tokens:\n",
    "    tokens.append(token)\n",
    "    segment_ids.append(0)\n",
    "tokens.append(\"[SEP]\")\n",
    "segment_ids.append(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \n",
    "    # token can appear in multiple documents. E.g.\n",
    "    #  Doc: the man went to the store and bought a gallon of milk\n",
    "    #  Span A: the man went to the\n",
    "    #  Span B: to the store and bought\n",
    "    #  Span C: and bought a gallon of"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158\n",
      "124\n",
      "158\n",
      "124\n",
      "['Architecturally,', 'the', 'school', 'has', 'a', 'Catholic', 'character.', 'Atop', 'the', 'Main', \"Building's\", 'gold', 'dome', 'is', 'a', 'golden', 'statue', 'of', 'the', 'Virgin', 'Mary.', 'Immediately', 'in', 'front', 'of', 'the', 'Main', 'Building', 'and', 'facing', 'it,', 'is', 'a', 'copper', 'statue', 'of', 'Christ', 'with', 'arms', 'upraised', 'with', 'the', 'legend', '\"Venite', 'Ad', 'Me', 'Omnes\".', 'Next', 'to', 'the', 'Main', 'Building', 'is', 'the', 'Basilica', 'of', 'the', 'Sacred', 'Heart.', 'Immediately', 'behind', 'the', 'basilica', 'is', 'the', 'Grotto,', 'a', 'Marian', 'place', 'of', 'prayer', 'and', 'reflection.', 'It', 'is', 'a', 'replica', 'of', 'the', 'grotto', 'at', 'Lourdes,', 'France', 'where', 'the', 'Virgin', 'Mary', 'reputedly', 'appeared', 'to', 'Saint', 'Bernadette', 'Soubirous', 'in', '1858.', 'At', 'the', 'end', 'of', 'the', 'main', 'drive', '(and', 'in', 'a', 'direct', 'line', 'that', 'connects', 'through', '3', 'statues', 'and', 'the', 'Gold', 'Dome),', 'is', 'a', 'simple,', 'modern', 'stone', 'statue', 'of', 'Mary.']\n",
      "['architectural', '##ly', ',', 'the', 'school', 'has', 'a', 'catholic', 'character', '.', 'atop', 'the', 'main', 'building', \"'\", 's', 'gold', 'dome', 'is', 'a', 'golden', 'statue', 'of', 'the', 'virgin', 'mary', '.', 'immediately', 'in', 'front', 'of', 'the', 'main', 'building', 'and', 'facing', 'it', ',', 'is', 'a', 'copper', 'statue', 'of', 'christ', 'with', 'arms', 'up', '##rai', '##sed', 'with', 'the', 'legend', '\"', 've', '##ni', '##te', 'ad', 'me', 'om', '##nes', '\"', '.', 'next', 'to', 'the', 'main', 'building', 'is', 'the', 'basilica', 'of', 'the', 'sacred', 'heart', '.', 'immediately', 'behind', 'the', 'basilica', 'is', 'the', 'gr', '##otto', ',', 'a', 'marian', 'place', 'of', 'prayer', 'and', 'reflection', '.', 'it', 'is', 'a', 'replica', 'of', 'the', 'gr', '##otto', 'at', 'lou', '##rdes', ',', 'france', 'where', 'the', 'virgin', 'mary', 'reputed', '##ly', 'appeared', 'to', 'saint', 'bern', '##ade', '##tte', 'so', '##ub', '##iro', '##us', 'in', '1858', '.', 'at', 'the', 'end', 'of', 'the', 'main', 'drive', '(', 'and', 'in', 'a', 'direct', 'line', 'that', 'connects', 'through', '3', 'statues', 'and', 'the', 'gold', 'dome', ')', ',', 'is', 'a', 'simple', ',', 'modern', 'stone', 'statue', 'of', 'mary', '.']\n"
     ]
    }
   ],
   "source": [
    "# tok_to_orig_index: the orignal index for the token\n",
    "print(len(tok_to_orig_index))\n",
    "print(len(orig_to_tok_index))\n",
    "print(len(all_doc_tokens))\n",
    "print(len(example.doc_tokens))\n",
    "print(example.doc_tokens)\n",
    "print(all_doc_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "158"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_span.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import _check_is_max_context\n",
    "\n",
    "for i in range(doc_span.length):\n",
    "    split_token_index = doc_span.start + i\n",
    "    # print(split_token_index)\n",
    "    token_to_orig_map[len(tokens)] = tok_to_orig_index[split_token_index]\n",
    "    # print(doc_span_index,split_token_index)\n",
    "    is_max_context = _check_is_max_context(doc_spans, doc_span_index,\n",
    "                                            split_token_index)\n",
    "    token_is_max_context[len(tokens)] = is_max_context\n",
    "    tokens.append(all_doc_tokens[split_token_index])\n",
    "    segment_ids.append(1)\n",
    "    \n",
    "tokens.append(\"[SEP]\")\n",
    "segment_ids.append(1)\n",
    "input_ids = tokenizer.convert_tokens_to_ids(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "176\n",
      "158\n",
      "158\n",
      "158\n",
      "176\n"
     ]
    }
   ],
   "source": [
    "print(len(tokens))\n",
    "print(len(token_is_max_context))\n",
    "print(len(token_to_orig_map))\n",
    "print(len(tok_to_orig_index))\n",
    "print(len(segment_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "# tokens are attended to.\n",
    "input_mask = [1] * len(input_ids)\n",
    "max_seq_length = 512\n",
    "# Zero-pad up to the sequence length.\n",
    "while len(input_ids) < max_seq_length:\n",
    "    input_ids.append(0)\n",
    "    input_mask.append(0)\n",
    "    segment_ids.append(0)\n",
    "\n",
    "assert len(input_ids) == max_seq_length\n",
    "assert len(input_mask) == max_seq_length\n",
    "assert len(segment_ids) == max_seq_length\n",
    "\n",
    "start_position = None\n",
    "end_position = None\n",
    "if is_training and not example.is_impossible:\n",
    "    # For training, if our document chunk does not contain an annotation\n",
    "    # we throw it out, since there is nothing to predict.\n",
    "    doc_start = doc_span.start\n",
    "    doc_end = doc_span.start + doc_span.length - 1\n",
    "    out_of_span = False\n",
    "    if not (tok_start_position >= doc_start and\n",
    "            tok_end_position <= doc_end):\n",
    "        out_of_span = True\n",
    "    if out_of_span:\n",
    "        start_position = 0\n",
    "        end_position = 0\n",
    "    else:\n",
    "        doc_offset = len(query_tokens) + 2\n",
    "        start_position = tok_start_position - doc_start + doc_offset\n",
    "        end_position = tok_end_position - doc_start + doc_offset\n",
    "if is_training and example.is_impossible:\n",
    "    start_position = 0\n",
    "    end_position = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.append(\n",
    "    InputFeatures(\n",
    "        unique_id=unique_id,\n",
    "        example_index=example_index,\n",
    "        doc_span_index=doc_span_index,\n",
    "        tokens=tokens,\n",
    "        token_to_orig_map=token_to_orig_map,\n",
    "        token_is_max_context=token_is_max_context,\n",
    "        input_ids=input_ids,\n",
    "        input_mask=input_mask,\n",
    "        segment_ids=segment_ids,\n",
    "        start_position=start_position,\n",
    "        end_position=end_position,\n",
    "        is_impossible=example.is_impossible))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer('.\\\\vocab\\\\vocab', do_lower_case=True, max_len=512) # for bert large\n",
    "from data import convert_examples_to_features\n",
    "train_features = convert_examples_to_features(\n",
    "                examples=train_examples,\n",
    "                tokenizer=tokenizer,\n",
    "                max_seq_length=512,\n",
    "                doc_stride=128,\n",
    "                max_query_length=64,\n",
    "                is_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101,\n",
       " 2054,\n",
       " 2003,\n",
       " 1999,\n",
       " 2392,\n",
       " 1997,\n",
       " 1996,\n",
       " 10289,\n",
       " 8214,\n",
       " 2364,\n",
       " 2311,\n",
       " 1029,\n",
       " 102,\n",
       " 6549,\n",
       " 2135,\n",
       " 1010,\n",
       " 1996,\n",
       " 2082,\n",
       " 2038,\n",
       " 1037,\n",
       " 3234,\n",
       " 2839,\n",
       " 1012,\n",
       " 10234,\n",
       " 1996,\n",
       " 2364,\n",
       " 2311,\n",
       " 1005,\n",
       " 1055,\n",
       " 2751,\n",
       " 8514,\n",
       " 2003,\n",
       " 1037,\n",
       " 3585,\n",
       " 6231,\n",
       " 1997,\n",
       " 1996,\n",
       " 6261,\n",
       " 2984,\n",
       " 1012,\n",
       " 3202,\n",
       " 1999,\n",
       " 2392,\n",
       " 1997,\n",
       " 1996,\n",
       " 2364,\n",
       " 2311,\n",
       " 1998,\n",
       " 5307,\n",
       " 2009,\n",
       " 1010,\n",
       " 2003,\n",
       " 1037,\n",
       " 6967,\n",
       " 6231,\n",
       " 1997,\n",
       " 4828,\n",
       " 2007,\n",
       " 2608,\n",
       " 2039,\n",
       " 14995,\n",
       " 6924,\n",
       " 2007,\n",
       " 1996,\n",
       " 5722,\n",
       " 1000,\n",
       " 2310,\n",
       " 3490,\n",
       " 2618,\n",
       " 4748,\n",
       " 2033,\n",
       " 18168,\n",
       " 5267,\n",
       " 1000,\n",
       " 1012,\n",
       " 2279,\n",
       " 2000,\n",
       " 1996,\n",
       " 2364,\n",
       " 2311,\n",
       " 2003,\n",
       " 1996,\n",
       " 13546,\n",
       " 1997,\n",
       " 1996,\n",
       " 6730,\n",
       " 2540,\n",
       " 1012,\n",
       " 3202,\n",
       " 2369,\n",
       " 1996,\n",
       " 13546,\n",
       " 2003,\n",
       " 1996,\n",
       " 24665,\n",
       " 23052,\n",
       " 1010,\n",
       " 1037,\n",
       " 14042,\n",
       " 2173,\n",
       " 1997,\n",
       " 7083,\n",
       " 1998,\n",
       " 9185,\n",
       " 1012,\n",
       " 2009,\n",
       " 2003,\n",
       " 1037,\n",
       " 15059,\n",
       " 1997,\n",
       " 1996,\n",
       " 24665,\n",
       " 23052,\n",
       " 2012,\n",
       " 10223,\n",
       " 26371,\n",
       " 1010,\n",
       " 2605,\n",
       " 2073,\n",
       " 1996,\n",
       " 6261,\n",
       " 2984,\n",
       " 22353,\n",
       " 2135,\n",
       " 2596,\n",
       " 2000,\n",
       " 3002,\n",
       " 16595,\n",
       " 9648,\n",
       " 4674,\n",
       " 2061,\n",
       " 12083,\n",
       " 9711,\n",
       " 2271,\n",
       " 1999,\n",
       " 8517,\n",
       " 1012,\n",
       " 2012,\n",
       " 1996,\n",
       " 2203,\n",
       " 1997,\n",
       " 1996,\n",
       " 2364,\n",
       " 3298,\n",
       " 1006,\n",
       " 1998,\n",
       " 1999,\n",
       " 1037,\n",
       " 3622,\n",
       " 2240,\n",
       " 2008,\n",
       " 8539,\n",
       " 2083,\n",
       " 1017,\n",
       " 11342,\n",
       " 1998,\n",
       " 1996,\n",
       " 2751,\n",
       " 8514,\n",
       " 1007,\n",
       " 1010,\n",
       " 2003,\n",
       " 1037,\n",
       " 3722,\n",
       " 1010,\n",
       " 2715,\n",
       " 2962,\n",
       " 6231,\n",
       " 1997,\n",
       " 2984,\n",
       " 1012,\n",
       " 102,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features[1].input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5179d32cf6ec497baf3f8a3ef987cc77c5d2dc691fdde20a56316522f61a7323"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
